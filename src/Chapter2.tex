\section{Getting started with Mastodon. Automated tracking.}

This tutorial is the starting point for new Mastodon users. 
It will walk you through basic operations in Mastodon, opening a dataset and creating a Mastodon project, automatically detect cells and link them, and show you how to use the main views of Mastodon.
We don't go into details, and will revisit the features we survey here later.

\subsection{The image data.}

\subsubsection{Exporting your image to \Bdv file format.}

Mastodon uses \wikilink{BigDataViewer}{\Bdv} (BDV) files as input images.
You need to prepare your images so that they can be opened in the \bdv.

BDV files are used more and more by several software projects in the Fiji ecosystem and beyond. 
This tutorial focuses on Mastodon not on BDV, however we will take a very small detour to explain what makes it fit and how to turn your images into this format. 
If you know already, you can skip this part, because we simply recapitulate what is being explained in the original \Bdv publication~\cite{bdv}.

For this tutorial we will use a ready-made dataset, in the adequate format, but it is a good idea to know how to export or create an image in such a format.
We lazily rely on the excellent \bdv documentation and point directly to the \bdv instructions to prepare your images, \eg depending on whether
\begin{itemize}
	\item they are \wikilink{BigDataViewer\#Exporting_from_ImageJ_Stacks}{opened as an ImageJ stack}, or
	\item they come from a \wikilink{BigDataViewer\#Integration_with_Fiji.27s_SPIMage_Processing_Tools}{SPIM processing pipeline}, or
	\item they come from the \wikilink{BigStitcher}{BigStitcher} plugin~\cite{BigStitcher}, \etc.
\end{itemize}

Once you have prepared your images for opening in the \bdv, you should have a \texttt{.xml} file and a possibly very large \texttt{.h5} file on your computer. The \texttt{.xml} file must be the output of the \bdv data preparation. It should start with the following lines:
\begin{minted}{xml}
<?xml version="1.0" encoding="UTF-8"?>
<SpimData version="0.2">
  <BasePath type="relative">.</BasePath>
  <SequenceDescription>
    <ImageLoader format="bdv.hdf5">
      <hdf5 type="relative">datasethdf5.h5</hdf5>
...
\end{minted}


\subsubsection{Key advantages of the \Bdv file format.}
\label{BDV_advantages}

The BDV file format solves mainly two challenges in image visualization and analysis, that arise with modern microscopy, namely:
\begin{itemize}
    
    \item Modern microscopes can generate images that are very large in size. Much larger that what can be fitted in RAM, even with the increase in computer power. It is now common to find single movies acquired on SPIM microscopes that are several TBs in size. Computers with several TBs of RAM are not so common.
    
    \item Multiple views of the same sample can be acquired, and they need to be visualized in the same viewer. The first use case is also the multi-view images generated by SPIM microscopes, but we can also think of correlative light-electron microscopy.
    
\end{itemize}

If we focus on the first challenge, you see that we need to stream the image data directly from the disk, instead of fully loading it into RAM. 
But at the same time, we need a tool that allows for interactive browsing of the data. 
The view must be responsive to the user input, and not block when it has to load the data from the file. 
The BDV file format offers a clever file format design that does this, coupled to a specialized viewer.
The image data are stored in small chunks corresponding to a neighborhood. As the viewer shows a slice through the image, the required chunks are loaded on demand and cached.
All the chunks are organized in a HDF5 file, which is like a file-system in a file\footnote{\href{https://en.wikipedia.org/wiki/Hierarchical_Data_Format}{\coloredlink{Hierarchical Data Format} on Wikipedia.}}, and accessing single chunks is fast with current computer hardware.
On top of this, the image is also stored as a multi-scale pyramid\footnote{\href{https://en.wikipedia.org/wiki/Pyramid_(image_processing)}{\coloredlink{Multi-scale pyramid} on Wikipedia.}}, to speed-up zooming and unzooming (Figure~\ref{fig:BDVchunks}).
The BDV display component exploits this file format in a clever way, and ensures that the view still answers to user interactions (mouse pan, zoom, clicks \etc) even if the chunks are not full loaded.

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{figures/BdvTikz-pyramidblocks.png}
    \caption{Illustration of the BDV file format storage strategy. The image is stored over several resolution levels (multi-scale pyramid) and in chunks.}
    \label{fig:BDVchunks}
\end{figure}

There are several implementations of this strategy, for instance in Imaris\footnote{\href{http://open.bitplane.com/Default.aspx?tabid=268}{\coloredlink{IMARIS 5.5 File Format Description (IMS).}}} and with the new file format N5\footnote{\href{https://github.com/saalfeldlab/n5}{\coloredlink{N5 API on GitHub.}}} proposed by the Saalfeld lab.
Some of them are inter-compatible.
We will pick the BDV file format all along this document. 
The \Bdv has proved its value and impact on our field.
For instance our previous work on cell lineaging in large images, \wikilink{MaMuT}{MaMuT}, is based on BDV~\cite{MaMuT}.



\subsubsection{The tutorial dataset.}

Mastodon was created specially because we needed to harness very big, multi-view images. We wanted to generate  comprehensive lineages and follow a large number of cells over a very long time.
This accumulation of inflated words is tied to the very large  - in objective disk space  occupation - images we deal with using modern microscopy tools. 
Such datasets might not be optimal for a first contact with Mastodon.
So just for this tutorial we will use a smaller dataset.
It is a small region cut into a movie following the development of a drosophila, acquired in Pavel Tomancak lab (MPI-CBG).
You can find it on Zenodo\footnote{\href{https://zenodo.org/record/3336346}{\coloredlink{https://zenodo.org/record/3336346}}} there: \href{https://doi.org/10.5281/zenodo.3336346}{\includegraphics[height=1.5\fontcharht\font`\B]{figures/zenodo3336346.png}}

It is a zip file that contains 3 files:
\begin{minted}{text}
    14M  datasethdf5.h5
   2.7K  datasethdf5.settings.xml
   8.7K  datasethdf5.xml
\end{minted}

The \texttt{.h5} file is the HDF5 file mentioned above, that contains the image data itself.
The \texttt{data\-sethdf5.xml} is a text file following the XML convention, specific to the BDV file format, that contains information about the the image data and metadata. 
When we want to open a BDV file, we point the reader to this file.
The \texttt{datasethdf5.settings.xml} is an optional file that stores user display parameters, such as channel colors, min and max display value, as well as bookmarks in the data. 
We refer you to the \wikilink{BigDataViewer\#Loading_and_Saving_Settings}{BDV documentation} about this file.
Mastodon uses this settings file to store that same information.

If you open this data in the \Bdv (in Fiji in the \menu{Plugins > BigDataViewer > Open XML/HDF5} menu), you should see something like in figure~\ref{fig:OpeningImage}. 
There is about 70 cells in each of the 30 time-points, arranged in a layer at the top of the sample. 
The deeper part of the sample (low Z coordinates) has some hazy, diffuse signal from which we cannot individualize cells.
As time progresses, the cells move towards the middle part and bottom (high Y coordinates) part of the image, and some of them move deeper in Z, initiating gastrulation.

The goal of this short tutorial is to track all these cells in Mastodon.

\begin{figure}
     \centering
         \includegraphics[width=0.3\textwidth]{figures/BDV-imageXY.png}
         \includegraphics[width=0.3\textwidth]{figures/BDV-imageXZ.png}
         \caption{The tutorial dataset opened in \Bdv, seen along XY (left) and XZ (right)}
     \label{fig:OpeningImage}
\end{figure}  




\subsection{Getting Mastodon.}

As of today, Mastodon is available as a preview. We are still working on adding and validating features.
Nonetheless the preview has everything we need to track these cells.
Also, Mastodon is independent of ImageJ or Fiji, it can operate as a standalone software. 
However we currently distribute it via Fiji, because the updater and the dependency management are so convenient. 
So the first thing to do is to grab Fiji\footnote{\href{http://fiji.sc/}{\coloredlink{http://fiji.sc/}}}, if you do not have it already.

Then launch the \wikilink{Updater}{Fiji updater} and once your Fiji is up to date, click on the \texttt{Manage update site} button.
We will add the \wikilink{Following_an_update_site}{add the Mastodon update site}.
You should find the Mastodon preview site in the list. 
Select it, update Fiji and restart it. 
After restarting, you should find the command \menu{Plugins > Mastodon (preview)} at the bottom of the menu.

\begin{center}
    \includegraphics[width=0.7\textwidth]{figures/Mastodon_UpdateSite.png}
\end{center}



\subsection{Creating a new Mastodon project.}

After launching the command, this plain, sober window appears.
\begin{center}
         \includegraphics[width=0.3\textwidth]{figures/Mastodon_MainWindow.png}
\end{center}

Click on new \texttt{new project}, and browse to the \texttt{datasethdf5.xml} file of the XML/HDF5 file pair of the tutorial dataset.
All the buttons that were grayed out should be enabled. 
Click on the \texttt{bdv} button.
A BDV window should appear and if it does everything is right.
\begin{center}
         \includegraphics[width=0.4\textwidth]{figures/Mastodon_BDV.png}
\end{center}

It is almost a regular BDV window and if you already know who to use it and the key bindings you should find your marks quickly.
The BDV view displays a \textit{slice} of the image through arbitrary orientation. 
Below we give the commands and key-bindings for navigation in a Mastodon-BDV window. 
They are indeed close to what is found in the standard \Bdv but some changes. 
\textbf{Please note:} You can reconfigure almost everything in Mastodon, as we will see later, including key-bindings.
In this tutorial and the next ones, the key-bindings we present are for the \texttt{Default} configuration.
In the table~\ref{tab:MastodonBDVNavigationKeys} you will find the key bindings to navigate through the image data.

\begin{table}[!htbp]
    \centering
    
    \caption{Default navigation key-bindings for Mastodon-BDV views.}

    \input{src/TableBDVNavigationKeys.tex}

    \label{tab:MastodonBDVNavigationKeys}
    \vspace{-10pt}

\end{table}

Now you want to save the project. 
Go back to the main window, and click on the \menu{save project} button.
This will create a single file, called for instance \texttt{drosophila\_crop.mastodon} file. 
This file is actually a zip file that contains the tracks and \textit{links} to the image data.
The image data is kept separate from the Mastodon file, which allows for using it with another software, independently. 
So if you want to transfer or move a full Mastodon project, you need to take the \texttt{.mastodon} file and all the \texttt{.xml} and \texttt{.h5} files from the \Bdv dataset.

Next time you want to open this project, just click on the \menu{load project} button and point the file browser to the \texttt{.mastodon} file.
The image data will be loaded along with the lineages.



\subsection{Detecting cells.}

\begin{wrapfigure}{r}{0.4\textwidth}
    \centering
         \includegraphics[width=0.4\textwidth]{figures/Mastodon_DetectionWizard_01.png}
\end{wrapfigure}

We want to track automatically all the cells in this dataset, and the first step is therefore to detect them.
Mastodon ships a wizard to perform cell detection. 
It is very much inspired by the \wikilink{TrackMate}{TrackMate} GUI, and if you know this software you will find your marks here.
Also, the algorithms are very close to what was in TrackMate~\cite{TrackMate}, but they have been heavily optimized for Mastodon.

The detection wizard can be launched from the \menu{Plugins > Tracking > Detection...} menu item. 
You should have a window like this one appearing:

Like for TrackMate, the automated tracking user interface uses \textit{wizards} to enter parameters, select algorithms, \textit{etc.}
You can navigate back and forth with the \menu{\smallimg{arrow_right.png} Next} and  \menu{\smallimg{arrow_left.png} Previous} buttons. 
The \menu{\smallimg{book.png} Log} button will bring an independent panel where all the activity in the wizard are logged as text. 

This first panel allows for selecting the target \textit{source} on which the detection will be run. 
Since we use the \bdv for images, a channel or a view is stored and displayed as a source. 
A source can have multiple resolutions stored, as explain in paragraph~\ref{BDV_advantages}, but for the data used in this tutorial this is not the case. The sources are nicknamed 'setups' in this panel.
They are numbered from 0 and can be selected from the drop-down list. 
Below the list we try to display the metadata we could retrieve from the \bdv file.
Just pick the first and only channel, and click \menu{\smallimg{arrow_right.png} Next}.

You can now choose to operate only on a rectangular ROI in the image. 
If you check the \textbf{Process only a ROI} button, new controls appear in the panel, and a ROI is drawn into an open BDV view (a new one is created if one is not opened).
The ROI is painted as a wire-frame box, green for vertices that point towards the camera from the displayed slice, and purple for vertices that points away from the camera, below the displayed slice.
The intersection of the ROI box with the displayed slice is painted with a purple semi-transparent overlay, with a white dotted line as borders.
You can control the ROI bounds wither with the controls in the panel, or by directly dragging the ROI corners in the BDV view.
Time bounds can also be set this way. In our case we want to segment the full image over all time-points, so leave the \textbf{Process only a ROI} button unchecked.

You cannot have non-rectangular ROI in Mastodon. Nonetheless they are super useful as is. 
You can for instance combine several detection steps using different parameters in different region of your image. Or different time interval.

\begin{center}
         \includegraphics[height=0.25\textheight]{figures/Mastodon_ROIpanel.png}
         \includegraphics[height=0.25\textheight]{figures/Mastodon_ROIBDV.png}
\end{center}

The next panel lets you choose the detector you want to use.
In vanilla Mastodon, three detectors are available.
Right now, we will use the default one, the \textbf{DoG detector}, which should be good enough for most cases.
DoG means 'difference-of-Gaussians'.
It is an efficient approximation of the LoG ('Laplacian of Gaussian') filter, and there is also a detector in Mastodon based on the latter.

These detectors excel at finding roundish structures in the image that are bright over a dark background.
The structures must have a shape somewhat close to a sphere, but they can accommodate a lot of variability.
As a rule of thumb, if you can define rough estimate of the radius of these structures, they are eligible to be picked up by our detectors. 
This also implies that in Mastodon, we cannot segment complex shapes, or object labeled by their contour (\textit{e.g.} cell membranes), or even exploit these shapes to have an accurate measurements of the volume. 
This is an important limitation of Mastodon.

For now, select the \textbf{DoG detector} and click \menu{\smallimg{arrow_right.png} Next}.

Here is briefly  how it works.
The LoG detector, and its approximation the DoG detector, is the best detector for Gaussian-like particles in the presence of noise. 
It is based on applying a Laplacian of Gaussian (LoG) filter on the image and looking for local maxima. The result is obtained by summing the second order spatial derivatives of the gaussian- filtered image, and normalizing for scale.
Local maxima in the filtered image yields spot detections. 
Each detected spot is assigned a \textbf{quality} value by taking the local maxima value in the filtered image.
So by properties of the LoG filter, this quality value is larger for :
\begin{myitemize}
    \item bright spots;
    \item spots which diameter is close to the specified diameter.
\end{myitemize}

So the DoG detector requires only two parameters: the estimated diameter of the object we want to detect, and a threshold on the quality value, that will help separating spurious detections from real ones. 
The panel you are presented let you specify these parameters, and preview the resulting detection.
Try with 10 pixels for \textbf{Estimated diameter} and 0 for the \textbf{Quality threshold}.
The click on the \menu{\smallimg{led-icon-eye-green.png} Preview} button.
A preview panel should open shortly, showing detection results on the current frame

\begin{center}
         \includegraphics[height=0.25\textheight]{figures/Mastodon_DoGconfig1.png}
         \includegraphics[height=0.25\textheight]{figures/Mastodon_DoGconfig2.png}
\end{center}

These values are close but not quite.
You can see that the diameter value is too small to properly grasps the elongated shape of the cells along Z (the BDV view on the left panel above is rotated to show a YZ plane). 
Also the threshold value is too low, and some spurious detections are found below the epithelium.
These spots have a low quality, that manifests as a peak at low value in the quality histogram displayed on the configuration panel.
From the shape of the histogram, we can infer that a threshold value around 100 should work.
However we also need to change the diameter parameter, which will change the range of quality values.
After trial and errors, values around 15 pixels for the diameter and 400 for the threshold seem to work.

\begin{wrapfigure}{r}{0.4\textwidth}
    \centering
    \includegraphics[height=0.40\textwidth, trim=0.5cm 0.5cm .5cm .5cm, clip]{figures/Mastodon_DetectionResuts.png}
\end{wrapfigure}

Note that you can run the preview on any frame.
You just have to move the time slider on the preview window.
Once you are happy with the parameters, click on the \menu{\smallimg{arrow_right.png} Next} button.
All the frames specified in the ROI (if any) will be processed. 
In our case detection should conclude quickly and the following panel should appear:

We now have more than 1000 cells detected and this concludes the detection step.
Click on the \menu{\smallimg{accept-icon.png} Finish} button, and the wizard will disappear.

If you have complex images mixing several size of objects, or detection parameters that work for one part of the movie but not for another one, you could restart a new detection now, selecting for instance other parts of the movie with the ROI.
You can do this and more, but for this kind of approach, the \textbf{Advanced DoG detector} offers more configuration capabilities, that will review later.



\subsection{Linking cells.}

What we just did is the detection step. 
It yields one Mastodon spot per cell, but the notion of cell identity propagated over time is missing yet.
The particle linking step just does that.
A particle linking or tracking algorithm accepts a collection of spots, ordered by frames (time-points), and tries to link each spot to the next spot(s) in the next frame or so. 
All the spots you can reach by starting from one spot and navigating across links build a \textit{track}, and in our case it represents one cell (or any other object) followed over time. 
In most cases there is one spot per frame for a track, meaning that that a spot has at most one incoming link (spot from previous frame) and one outgoing kink (spot in next frame). 
But some algorithms can accommodate \textit{e.g.} dividing cells (2 outgoing links for the mother cell going to the two daughter cells) and merging events. There is a vast literature behind tracking algorithms, and it is an active domain of Research. A relatively recent paper compare implementation and list some pros and pitfalls of many of them~\cite{Chenouard2014}.

\subsubsection{Selecting target spots for linking.}

\begin{wrapfigure}{r}{0.3\textwidth}
    \centering
    \includegraphics[height=0.3\textwidth,trim=0.5cm .5cm .5cm .5cm,clip]{figures/Mastodon_LinkingWizard_01.png}
\end{wrapfigure}

Like for the detection step, linking in Mastodon happens in a wizard.
And also like for detection, the linking algorithms currently available in Mastodon are adapted from TrackMate. 

Launch the wizard from the GUI, with the \menu{Plugins > Tracking > Linking...} menu item. 
The first panel you are shown lets you select what spots to include in linking. 
There are two modes:
\begin{myitemize}
    \item Either you take all the spots between a start and and end frame. By default, all frames are selected.
    \item Either you specify you want to link only the spots that are in the selection. 
    This mode offers a lot of flexibility when facing complicated cases. 
    It is best use along with the selection creator, that we will introduce later in this manual.
\end{myitemize}
For now, just leave the parameters as they are, which will include all spots in the linking process, and click next.
You can now choose between several linking algorithms. 

\subsubsection{Available linking algorithms in Mastodon.}

In Mastodon, they fall mainly in two categories.

The first two LAP trackers are based on the \textbf{Linear Assignment Problem (LAP) framework}, first developed by Jaqaman \textit{et al.}~\cite{Jaqaman2008}, with important differences from the original paper described elsewhere~\cite{TrackMate}. We focused on this method for it gave us a lot of flexibility and it can be configured easily to handle many cases. You can tune it to allow splitting events, where a track splits in two, for instance following a cell that encounters mitosis. Merging events are handled too in the same way. More importantly are gap-closing events, where a spot disappear for one frame (because it moves out of focus, because detection failed, ...) but the track manages to recuperates and connect with reappearing spots later.

In Mastodon the LAP algorithms exists in two flavors: a simple one and a not simple one. There are again the same, but the simple ones propose fewer configuration options and a thus more concise configuration panel. In short:
\begin{myitemize}
    \item The simple one only allows to deal with gap-closing events, and prevent splitting and merging events to be detected. Also, the costs to link two spots are computed solely based on their respective distance.
    
    \item The not simple one allows to detect any kind of event, so if you need to build tracks that are splitting or merging, you must go for this one. If you want to forbid the detection of gap-closing events, you want to use it as well. Also, you can alter the cost calculation to disfavor the linking of spots that have very different feature values.
\end{myitemize}

The third tracker is called \textbf{Linear motion Kalman linker}.
It can deal specifically with linear motion, or particles moving with a roughly constant velocity.
This velocity does not need to be the same for all particles. 
It relies on the Kalman filter\footnote{\href{https://en.wikipedia.org/wiki/Kalman_filter}{\coloredlink{Kalman filter on Wikipedia.}}} to predict the most probable position of a particle undergoing (quasi) constant velocity movement.

\subsubsection{How to pick the right linking algorithm?}

The right choice of a particle linking algorithm is conditioned by the expected motion of the object you track.
As a rule of thumb, you can make a decision following these simple rules:
\begin{myitemize}
    
    \item If the objects you track are transported by an active process and have a motion for which the velocity vector changes slowly, then pick the  \textbf{Linear motion Kalman linker}.
    
    \item If the object motion is random (like in Brownian motion) or unknown, pick on the LAP linker. If the objects you track do not divide, nor merge, pick the \textbf{Simple LAP linker}.
    
    \item If the objects divide or merge, or if you want to specify linking costs based on numerical features (like spot mean intensity), then pick the \textbf{LAP linker}.
    
\end{myitemize}
In our case, we need the \textbf{Simple LAP linker}. Select it and click \menu{\smallimg{arrow_right.png} Next}.

\subsubsection{Running the Simple LAP linker.}

This linker only requires the specification of three parameters.

The first one is the \textbf{Max linking distance} during frame-to-frame linking. 
It is the distance beyond which linking a spot to another one in the next frame will be forbidden. 
For instance, if you know that your objects move by at most 5~{\textmu}m from one frame to the next, pick a value slightly larger, for instance 6~{\textmu}m.
Distances are expressed in whatever physical units the BDV dataset specified.
In our case it is pixels. 

\begin{center}
    \includegraphics[height=0.25\textheight,trim=0.5cm .5cm .5cm .5cm,clip]{figures/Mastodon_LinkingWizard_03.png}
\end{center}

In practical cases, it can happen that the detection step might miss an object in some frames, then detect again later.
These gaps will result in generating several small tracks for a single objects, which is one of the main course of spurious results when analyzing tracks. 
The simple LAP linker can bridge over missed detections.
It does so by inspecting small tracks that results from frame-to-frame linking, and tries to connect the end of one with the beginning of another one.
The last two parameters of the linker specifies how they are bridged. 
The \textbf{Max gap-closing distance} specifies how far can we look for candidates when we try to bridge the end of a track with the beginning of another one.
The \textbf{Max frame gap} specifies how far they can be in time. 
For instance a Max-frame-gap of 2 means that we can bridge the end of a track at frame $t$ with the beginning of a track at frame $t+2$.
Which results of bridging over detections missed by no more than 1 frame.

In our case, the default parameters turn to work fine. 
Click \menu{\smallimg{arrow_right.png} Next} and the linking will proceed. 
Click on the \menu{\smallimg{accept-icon.png} Finish} button to end the tracking process.
If you have a BDV window opened, it should be updated with the tracking results, like in figure~\ref{fig:Tracking results}.
By default the tracks are represented by colored lines, extending backward in time.
Points in tracks that are close to the current time-point are green and fade to ref for points that are far back in time.
When you change the Z focus, the spots are painted as circle of radius corresponding the the intersection of the sphere with the current Z-plane. 
When the spot sphere does not interest with current Z-plane, it is painted as a small dots.
The points of the track away in time that are not close to the current Z-slice are faded away.
We will see later how to customize the display of tracks. 
 
 
\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth,trim=0.5cm .5cm .5cm .5cm,clip]{figures/Mastodon_LinkingResults.png}
     \caption{How tracking results are displayed in BDV views.}
     \label{fig:Tracking results}
\end{figure}  
 
\subsection{Wrapping up.} 

This concludes our first tutorial on automated tracking with Mastodon.
To continue with the next chapter, save the project with the tracks you just generated.

As you can see, after creating a project from a BDV file, the process consists mainly in running in succession the two wizards, one for detection, one for particle linking. 
Even if they provide fully automated tracking, Mastodon is made for interacting with the data as you generate it. 
We will see in a next section how to manually edit a spot, a link or a track even at the finest granularity.
But keep in mind that the tools we quickly surveyed can be used interactively too. 
First the wizards let you go back to change a tracking parameter and check how the results are improved or not. 
Second, because you can specify a region-of-interest (ROI) in the detection step, and select the spots you want to track in the linking step, several runs of these wizard can be combined on different parts of the same image, to accommodate \textit{e.g.} for changing image quality over time, or cell shape over time. 
Mastodon aims at being the workbench for tracking that will get you to the results whatever the image.
